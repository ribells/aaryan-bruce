{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onrJdkjPbzd1"
   },
   "source": [
    "# A few simple corpus-driven approaches to narrative analysis and generation\n",
    "\n",
    "By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "This notebook is a fast introduction to a few techniques for working with narrative corpora. By \"narrative corpora,\" I mean pre-existing bodies of text that mostly contain the texts of narratives. In particular, we're going to use Mark Riedl's [WikiPlots corpus](https://github.com/markriedl/WikiPlots), which has the titles and plot summaries of more than one hundred thousand movies, books, television shows and other media from Wikipedia.\n",
    "\n",
    "The notebook takes you through using [spaCy](http://spacy.io) to extract words, noun chunks, parts of speech and entities from the text and then sew them back together with [Tracery](http://tracery.io). It then shows how to use [Markovify](https://github.com/jsvine/markovify) to create new narratives from existing narrative text, and how to prepare the narratives for use as a training corpus for a large pre-trained language model like GPT-2.\n",
    "\n",
    "The code is written in Python, but you don't really need to know Python in order to use the notebook. Everything's pre-written for you, so you can just execute the cells, making small changes to the code as needed. Even if the notebook itself doesn't end up being useful to you, hopefully it spurs a few ideas that you can take with you into your practice as a storyteller and/or programmer.\n",
    "\n",
    "If you're running this code on Binder, you should be good to go. Just keep on executing the cells below. If you're running this notebook on Google Colab, you'll need to run the following cells to install the necessary libraries and download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "B25UZm3gbzd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markovify\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/e2/d0f7df1d6cc9e0b5ab6018d5d302caa1639fe702f492362baac794b6e578/markovify-0.9.4.tar.gz\n",
      "Collecting unidecode (from markovify)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/5b/7603add7f192252916b85927263b598c74585f82389e6e42318a6278159b/Unidecode-1.3.4-py3-none-any.whl (235kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 11.3MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: markovify\n",
      "  Building wheel for markovify (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for markovify: filename=markovify-0.9.4-cp37-none-any.whl size=18611 sha256=c9846f40478e662ddb6caa1d306825ca5bdb7daaad10b936d49e641261dd6a1d\n",
      "  Stored in directory: /Users/ribells/Library/Caches/pip/wheels/92/3e/48/4b42ce0f6985e205a5d21f2a47e6fc2c156e10cbf3fec356ea\n",
      "Successfully built markovify\n",
      "Installing collected packages: unidecode, markovify\n",
      "Successfully installed markovify-0.9.4 unidecode-1.3.4\n",
      "Collecting tracery\n",
      "  Downloading https://files.pythonhosted.org/packages/96/9a/7a7ffa8ace49ac6a3f422fdd83caf8d0ad0d132f01389f02845379033a7b/tracery-0.1.1.tar.gz\n",
      "Building wheels for collected packages: tracery\n",
      "  Building wheel for tracery (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tracery: filename=tracery-0.1.1-cp37-none-any.whl size=7696 sha256=aefb0195141b60d3021c16fbbfbbf28ad5388dc77a40473ccf7388fbfd9f41f1\n",
      "  Stored in directory: /Users/ribells/Library/Caches/pip/wheels/22/ba/09/8341e10777b9d2bf6d5414d3c9627b7812f5bedce4b3edacac\n",
      "Successfully built tracery\n",
      "Installing collected packages: tracery\n",
      "Successfully installed tracery-0.1.1\n",
      "Collecting spacy==2.3.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/66/facc29889e0be6cceb64cbb9d4dff45a3defee79b333d41c8a2597eb6b5e/spacy-2.3.2-cp37-cp37m-macosx_10_9_x86_64.whl (10.0MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0MB 9.6MB/s eta 0:00:01    |███████████████████▉            | 6.2MB 9.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6 (from spacy==2.3.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy==2.3.2) (2.22.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy==2.3.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/7a/5e5e54a6368974bcb2a7ffc7013d012b47b084f3abea6978c9265f40750d/murmurhash-1.0.7-cp37-cp37m-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from spacy==2.3.2) (41.4.0)\n",
      "Collecting catalogue<1.1.0,>=0.0.7 (from spacy==2.3.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
      "Collecting srsly<1.1.0,>=1.0.2 (from spacy==2.3.2)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/be/1b94141d3d65ddf86a7e63daeba72fbbf054ad6ad3a5d42b296f68c70030/srsly-1.0.5-cp37-cp37m-macosx_10_9_x86_64.whl (177kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 22.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2 (from spacy==2.3.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/94/77ca1d1efad56008ee567fe2f23d94ad5be86de32b55ed8a6a15a0782ca8/cymem-2.0.6-cp37-cp37m-macosx_10_9_x86_64.whl\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy==2.3.2)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/ac/f751cd2450ac529d23a9dd1aa1c7a142f8b9555ad3e735411d0a641391a3/preshed-3.0.6-cp37-cp37m-macosx_10_9_x86_64.whl (104kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 20.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc==7.4.1 (from spacy==2.3.2)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/3f/9cee434ca42cd7902c1369038daf6c78e06bc101750aa75c6eed1a7bdf03/thinc-7.4.1-cp37-cp37m-macosx_10_9_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 11.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.38.0 (from spacy==2.3.2)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/c4/d15f1e627fff25443ded77ea70a7b5532d6371498f9285d44d62587e209c/tqdm-4.64.0-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 5.1MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0 (from spacy==2.3.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/77/736fa303d2efb5b640aad8abad323c23c83c184ce95c4df25e8a8e435d2e/wasabi-0.9.1-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy==2.3.2) (1.17.2)\n",
      "Collecting blis<0.5.0,>=0.4.0 (from spacy==2.3.2)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d8/f0be9d8ebec9cbeea1427de6ac0ecc919c0bfe881eff2d2965dbc310ca8b/blis-0.4.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.0MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0MB 4.6MB/s eta 0:00:01     |█▉                              | 235kB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.2) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.2) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.2) (7.2.0)\n",
      "Installing collected packages: plac, murmurhash, catalogue, srsly, cymem, preshed, tqdm, wasabi, blis, thinc, spacy\n",
      "  Found existing installation: tqdm 4.36.1\n",
      "    Uninstalling tqdm-4.36.1:\n",
      "      Successfully uninstalled tqdm-4.36.1\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.6 murmurhash-1.0.7 plac-1.1.3 preshed-3.0.6 spacy-2.3.2 srsly-1.0.5 thinc-7.4.1 tqdm-4.64.0 wasabi-0.9.1\n",
      "Collecting en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\n",
      "\u001b[K     |████████████████████████████████| 12.1MB 9.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.6)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.22.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.64.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.17.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (41.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.2.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-cp37-none-any.whl size=12047110 sha256=15770a3772feeec1800eca831c18853be9cbfb68fdcc8332080b860ba45b7ce7\n",
      "  Stored in directory: /private/var/folders/zf/49prmn_s7s5861rytkmr3_080000gn/T/pip-ephem-wheel-cache-ybofqwx5/wheels/2b/3f/41/f0b92863355c3ba34bb32b37d8a0c662959da0058202094f46\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 4014k  100 4014k    0     0  2505k      0  0:00:01  0:00:01 --:--:-- 2505k\n"
     ]
    }
   ],
   "source": [
    "!pip install markovify\n",
    "!pip install tracery\n",
    "!pip install spacy==2.3.2\n",
    "!python -m spacy download en_core_web_sm\n",
    "!curl -L -O https://github.com/aparrish/corpus-driven-narrative-generation/raw/master/romcom_plot_sentences.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uq2Plhsibzd4"
   },
   "source": [
    "## Loading the corpus\n",
    "\n",
    "The first step is to get the narrative corpus into the program. Because WikiPlots is so big, we're actually going to be working with a smaller subset: only the plot summaries for romantic comedy movies. The subcorpus was made using [this notebook on creating a subcorpus of WikiPlots](https://github.com/aparrish/corpus-driven-narrative-generation/blob/master/creating-a-wikiplots-subcorpus.ipynb), which you can consult if you want to make your own with a different subset of WikiPlots.\n",
    "\n",
    "The corpus we're working with takes the form of a TSV file (\"tab separated values\"), with each line containing the title of the movie, a number indicating where in the plot summary the sentence for this line occurs, the total number of sentences in the summary, and the actual text of the sentence. The following cell loads the data into a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z--7hwZtbzd5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for line in open(\"romcom_plot_sentences.tsv\"):\n",
    "    line = line.strip()\n",
    "    items = line.split(\"\\t\")\n",
    "    sentences.append(\n",
    "        {'title': items[0],\n",
    "         'index': int(items[1]),\n",
    "         'total': int(items[2]),\n",
    "         'text': items[3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHa69C9Dbzd6"
   },
   "source": [
    "Just to make sure it worked, we'll print out a random sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oWCusEXkbzd6"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z7MNFmt3bzd7",
    "outputId": "28207bce-ef4e-44c6-d151-eafa832010ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Merrily We Live',\n",
       " 'index': 5,\n",
       " 'total': 25,\n",
       " 'text': 'Further attempts to convince mrs Kilbourne to get rid of this latest tramp are blissfully ignored.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hF3n-nsdbzd7"
   },
   "source": [
    "Note: You can make your own corpus that works with the code in this notebook by exporting your data in TSV format with one line per sentence, with columns for the following:\n",
    "\n",
    "* `title`: the title of the work that the sentence comes from\n",
    "* `index`: the index of the sentence in the work\n",
    "* `total`: the total number of sentences in the work\n",
    "* `text`: the text of the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PF1UvDLbzd8"
   },
   "source": [
    "## Natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKwUlurLbzd9"
   },
   "source": [
    "To get an idea of what's happening in the text of the plots, we can do a bit of Natural Language Processing. I cover just the bare essentials in this notebook. [Here's a more in-depth tutorial that I wrote](https://github.com/aparrish/rwet/blob/master/nlp-concepts-with-spacy.ipynb).\n",
    "\n",
    "Most natural language processing is done with the aid of third-party libraries. We're going to use one called spaCy. To use spaCy, you first need to install it (i.e., download the code and put it in a place where Python can find it) and download the language model. (The language model contains statistical information about a particular language that makes it possible for spaCy to do things like parse sentences into their constituent parts.)\n",
    "\n",
    "Run the following cell to load spaCy's model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OgTEULYmbzd-"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMm_uQJibzd-"
   },
   "source": [
    "(This could also take a while–the model is potentially very large and your computer needs to load it from your hard drive and into memory. When you see a `[*]` next to a cell, that means that your computer is still working on executing the code in the cell.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qrZdI4obzd-"
   },
   "source": [
    "Right off the bat, the spaCy library gives us access to a number of interesting units of text:\n",
    "\n",
    "* All of the sentences (`doc.sents`)\n",
    "* All of the words (`doc`)\n",
    "* All of the \"named entities,\" like names of places, people, #brands, etc. (`doc.ents`)\n",
    "* All of the \"noun chunks,\" i.e., nouns in the text plus surrounding matter like adjectives and articles\n",
    "\n",
    "The cell below, we extract these into variables so we can play around with them a little bit. (Parsing sentences is hungry work and the following cell will take a while to execute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-qPkrk9Ubzd-",
    "outputId": "6b87d6a6-2698-4a33-8a07-5171d9fa744e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28785\n",
      "100 28785\n",
      "200 28785\n",
      "300 28785\n",
      "400 28785\n",
      "500 28785\n",
      "600 28785\n",
      "700 28785\n",
      "800 28785\n",
      "900 28785\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "noun_chunks = []\n",
    "entities = []\n",
    "plot = []\n",
    "# only use 1000 sentences sampled at random by default; comment out this `for...`\n",
    "# uncomment the `for...` beneath to use every sentence in the corpus.\n",
    "for i, sent in enumerate(random.sample(sentences, 1000)):\n",
    "#for i, sent in enumerate(sentences):\n",
    "    if i % 100 == 0:\n",
    "        print(i, len(sentences))\n",
    "    doc = nlp(sent['text'])\n",
    "    plot.append(doc)\n",
    "    words.extend([w for w in list(doc) if w.is_alpha])\n",
    "    noun_chunks.extend(list(doc.noun_chunks))\n",
    "    entities.extend(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x1280 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "#define Matplotlib figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 16), dpi=80)\n",
    "\n",
    "#create simple line plot\n",
    "ax.plot([20, 20],[20, 200*.1])\n",
    "\n",
    "for p in range(0, 200):\n",
    "    parsed = []\n",
    "    parsed.extend([w for w in list(plot[p]) if w.is_alpha])\n",
    "    w_beg = 0\n",
    "    w_end = 0\n",
    "    \n",
    "    for i in range(0, len(parsed)):\n",
    "        #print(parsed[i].pos_)\n",
    "\n",
    "        w_end = w_end + len(parsed[i])\n",
    "\n",
    "        color = 'black'\n",
    "        if(parsed[i].pos_ == \"PRON\"):\n",
    "            color = 'pink'\n",
    "        elif(parsed[i].pos_ == \"VERB\"):\n",
    "            color = 'red'\n",
    "        elif(parsed[i].pos_ == \"CCONJ\"):\n",
    "            color = 'yellow'\n",
    "        elif(parsed[i].pos_ == \"PROPN\"):\n",
    "            color = 'green'\n",
    "        elif(parsed[i].pos_ == \"CCONJ\"):\n",
    "            color = 'blue'\n",
    "        elif(parsed[i].pos_ == \"AUX\"):\n",
    "            color = 'magenta'\n",
    "        elif(parsed[i].pos_ == \"ADV\"):\n",
    "            color = 'cyan'\n",
    "        elif(parsed[i].pos_ == \"DET\"):\n",
    "            color = 'brown'\n",
    "        elif(parsed[i].pos_ == \"NOUN\"):\n",
    "            color = 'purple'\n",
    "        elif(parsed[i].pos_ == \"ADP\"):\n",
    "            color = 'orange'\n",
    "        #add rectangle to plot\n",
    "        ax.add_patch(Rectangle((w_beg, (p*.1)), w_end, (p*.1)+.1, edgecolor = color,\n",
    "                 facecolor = color,\n",
    "                 fill=True,\n",
    "                 lw=5))\n",
    "        w_beg = w_end\n",
    "\n",
    "    ax.add_patch(Rectangle((w_beg, (p*.1)-.05), 500, (p*.1)+.1, edgecolor = 'white',\n",
    "             facecolor = 'white',\n",
    "             fill=True,\n",
    "             lw=5))\n",
    "        \n",
    "\n",
    "ax.add_patch(Rectangle((0, (p*.1)+.1), 500, 20, edgecolor = 'white',\n",
    "         facecolor = 'white',\n",
    "         fill=True,\n",
    "         lw=5))\n",
    "\n",
    "#display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjKCslRDbzd_"
   },
   "source": [
    "Just to make sure it worked, print out ten random words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ftvfNE2Abzd_",
    "outputId": "d935f3d6-82f0-4fd0-f1fd-23065d23cb3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then\n",
      "changed\n",
      "dating\n",
      "love\n",
      "the\n",
      "fails\n",
      "signs\n",
      "return\n",
      "marry\n",
      "it\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(words, 10):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAVp25zHbzd_"
   },
   "source": [
    "Ten random noun chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SkrsMuVdbzeA",
    "outputId": "f9190e87-3787-4467-a63d-9608a0df4943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a seemingly-deserted island\n",
      "Pride\n",
      "charlatan mystics\n",
      "a clean health record\n",
      "Paige's medical school ambitions\n",
      "Elizabeth\n",
      "the editor\n",
      "town\n",
      "Stephanie\n",
      "She\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(noun_chunks, 10):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvweRgb9bzeA"
   },
   "source": [
    "Ten random entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0Hy7qRSKbzeB",
    "outputId": "0c65b81d-61f9-4514-a940-a2764b985165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anne Marie\n",
      "Mumbai\n",
      "Grace\n",
      "Laida\n",
      "Isolde\n",
      "Jane\n",
      "Rod\n",
      "Lacuna\n",
      "Fraser\n",
      "Vogue\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(entities, 10):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LF7ZdHZibzeB"
   },
   "source": [
    "### Grammatical roles\n",
    "\n",
    "The parser included with spaCy can also give us information about the grammatical roles in the sentence. For example, the `.root.dep_` attribute of a noun chunk tells us whether that noun chunk is the subject of the sentence (\"nsubj\") or a direct object (\"dobj\") of the sentence. (See the \"Universal Dependency Labels\" of spaCy's [annotation specs](https://spacy.io/api/annotation) for more possible roles.) Using this information, we can make a list of sentence subjects and sentence objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sIRSWjMMbzeC"
   },
   "outputs": [],
   "source": [
    "subjects = [chunk for chunk in noun_chunks if chunk.root.dep_ == 'nsubj']\n",
    "objects = [chunk for chunk in noun_chunks if chunk.root.dep_ == 'dobj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qH9eTRx8bzeC",
    "outputId": "c66e183c-345b-4575-ff87-4d136faf6ca2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[he,\n",
       " Paige's father,\n",
       " This novel,\n",
       " you,\n",
       " a constrained life,\n",
       " Alexandra,\n",
       " Jack,\n",
       " the fantasy,\n",
       " he,\n",
       " her uncle]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(subjects, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-yKYrNWobzeC",
    "outputId": "e0d9d486-bcc8-49dd-a801-aeecc04bad21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[him,\n",
       " him,\n",
       " what,\n",
       " an old romance,\n",
       " his money,\n",
       " her,\n",
       " the tension,\n",
       " Mike's card,\n",
       " what,\n",
       " his pregnant girlfriend]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(objects, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jz4oEErabzeD"
   },
   "source": [
    "### Parts of speech\n",
    "\n",
    "The spaCy parser allows us to check what part of speech a word belongs to. In the cell below, we create four different lists—`nouns`, `verbs`, `adjs` and `advs`—that contain only words of the specified parts of speech. Using the `.tag_` attribute, we can easily get only particular forms of verbs; in this case, I'm just getting verbs that are in the past tense. ([There's a full list of part of speech tags here](https://spacy.io/docs/usage/pos-tagging#pos-tagging-english).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "odOZv8RObzeD"
   },
   "outputs": [],
   "source": [
    "nouns = [w for w in words if w.pos_ == \"NOUN\"]\n",
    "verbs = [w for w in words if w.pos_ == \"VERB\"]\n",
    "past_tense_verbs = [w for w in words if w.tag_ == 'VBD']\n",
    "adjs = [w for w in words if w.tag_ == \"JJ\"]\n",
    "advs = [w for w in words if w.pos_ == \"ADV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_14TV_4YbzeD"
   },
   "source": [
    "And now we can print out a random sample of any of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "t_Fuh8gSbzeD",
    "outputId": "2a867a4f-33ed-434c-db6e-d29216b6b76b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way\n",
      "outlaw\n",
      "notes\n",
      "couple\n",
      "dinner\n",
      "cast\n",
      "boyfriend\n",
      "house\n",
      "schemer\n",
      "father\n",
      "process\n",
      "allergy\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(nouns, 12): # change \"nouns\" to \"verbs\" or \"adjs\" or \"advs\" to sample from those lists!\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImXK3jc8bzeE"
   },
   "source": [
    "### Entity types\n",
    "\n",
    "The parser in spaCy not only identifies \"entities\" but also assigns them to a particular type. [See a full list of entity types here.](https://spacy.io/docs/usage/entity-recognition#entity-types) Using this information, the following cell builds lists of the people, locations, and times mentioned in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "M9gJGAlQbzeE"
   },
   "outputs": [],
   "source": [
    "people = [e for e in entities if e.label_ == \"PERSON\"]\n",
    "locations = [e for e in entities if e.label_ == \"LOC\"]\n",
    "times = [e for e in entities if e.label_ == \"TIME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE-HX81QbzeE"
   },
   "source": [
    "And then you can print out a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9qdWIlyYbzeF",
    "outputId": "746bc76f-92a0-445f-b381-262b6351482c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Later that evening\n",
      "that night\n",
      "one night\n",
      "The next morning\n",
      "one night\n",
      "the night\n",
      "the middle of the night\n",
      "3 AM\n",
      "the following morning\n",
      "that evening\n",
      "only a few hours\n",
      "the night\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(times, 12): # change \"times\" to \"people\" or \"locations\" to sample those lists\n",
    "    print(item.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VGwLAAgbzeF"
   },
   "source": [
    "### Finding the most common\n",
    "\n",
    "We won't go too deep into text analysis in this tutorial, but it's useful to be able to do the most fundamental task in text analysis: finding the things that are most common. The code to do this task looks like the following, which gives us a way to look up how often any word occurs in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XU_qUW5ebzeF"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter([w.text for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "sDMbdOeNbzeG",
    "outputId": "d4557f2f-63fa-4abd-bbb3-245f8ba17dde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count['lost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPIN0WV4bzeG"
   },
   "source": [
    "... and also tells us which words are most common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OEPStA6CbzeG",
    "outputId": "0cefe319-d3f4-406a-9a95-50950b8e7661"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 965),\n",
       " ('to', 786),\n",
       " ('and', 696),\n",
       " ('a', 506),\n",
       " ('her', 359),\n",
       " ('of', 344),\n",
       " ('is', 330),\n",
       " ('his', 325),\n",
       " ('in', 286),\n",
       " ('he', 273),\n",
       " ('that', 258),\n",
       " ('with', 240)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XaIZpZ4bzeG"
   },
   "source": [
    "You can make a counter for any of the other lists we've worked with using the same syntax. Just make up a unique variable name on the left of the `=` sign and put the name of the list you want to count in the brackets to the right (replacing `words`). E.g., to find the most common people:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "g2PhznW6bzeG"
   },
   "outputs": [],
   "source": [
    "people_count = Counter([w.text for w in people])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZOfpAn1XbzeH",
    "outputId": "539a59f7-b6b2-451d-f1f9-0e41c3591b15",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 14),\n",
       " ('Tom', 13),\n",
       " ('Sam', 11),\n",
       " ('Anna', 11),\n",
       " ('Peter', 11),\n",
       " ('Kate', 11),\n",
       " ('George', 10),\n",
       " ('Joe', 9),\n",
       " ('Adam', 9),\n",
       " ('Dick', 8),\n",
       " ('Ryan', 8),\n",
       " ('Andy', 8)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_count.most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMfbC3v7bzeH"
   },
   "source": [
    "The most common past-tense verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8_jlchwMbzeH"
   },
   "outputs": [],
   "source": [
    "vbd_count = Counter([w.text for w in past_tense_verbs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IziDh6F1bzeH",
    "outputId": "50b4cab0-f17e-4882-edfe-ea5ba8b56331"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('had', 36),\n",
       " ('was', 30),\n",
       " ('were', 9),\n",
       " ('did', 8),\n",
       " ('married', 4),\n",
       " ('gave', 4),\n",
       " ('happened', 3),\n",
       " ('went', 3),\n",
       " ('ended', 3),\n",
       " ('broke', 3),\n",
       " ('died', 3),\n",
       " ('told', 3)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vbd_count.most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52K5LODebzeI"
   },
   "source": [
    "### Writing to a file\n",
    "\n",
    "The following cell defines a function for writing data from a `Counter` object to a file. The file is in \"tab-separated values\" format, which you can open using most spreadsheet programs. Execute it before you continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aRIEc4nabzeI"
   },
   "outputs": [],
   "source": [
    "def save_counter_tsv(filename, counter, limit=1000):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        outfile.write(\"key\\tvalue\\n\")\n",
    "        for item, count in counter.most_common():\n",
    "            outfile.write(item.strip() + \"\\t\" + str(count) + \"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhWiflu7bzeI"
   },
   "source": [
    "Now, run the following cell. You'll end up with a file in the same directory as this notebook called `100_common_words.tsv` that has two columns, one for the words and one for their associated counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "btietQWIbzeI"
   },
   "outputs": [],
   "source": [
    "save_counter_tsv(\"100_common_words.tsv\", word_count, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v44wvNmLbzeJ"
   },
   "source": [
    "Try opening this file in Excel or Google Docs or Numbers!\n",
    "\n",
    "If you want to write the data from another `Counter` object to a file:\n",
    "\n",
    "* Change the filename to whatever you want (though you should probably keep the `.tsv` extension)\n",
    "* Replace `word_count` with the name of any of the `Counter` objects we've made in this sheet and use it in place of `word_count`\n",
    "* Change the number to the number of rows you want to include in your spreadsheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z4FBEQUbzeJ"
   },
   "source": [
    "### When do things happen in this text?\n",
    "\n",
    "Here's another example. Using the `times` entities, we can make a spreadsheet of how often particular \"times\" (durations, times of day, etc.) are mentioned in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1MexULpRbzeJ"
   },
   "outputs": [],
   "source": [
    "time_counter = Counter([e.text.lower().strip() for e in times])\n",
    "save_counter_tsv(\"time_count.tsv\", time_counter, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF6zM-v_bzeK"
   },
   "source": [
    "Do the same thing, but with people:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3zmS3ADSbzeK"
   },
   "outputs": [],
   "source": [
    "people_counter = Counter([e.text.lower() for e in people])\n",
    "save_counter_tsv(\"people_count.tsv\", people_counter, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fJ_vwUQbzeK"
   },
   "source": [
    "### Generating stories from a corpus and Tracery grammars\n",
    "\n",
    "Once you've isolated entities and parts of speech, you can recombine them in interesting ways. One is to use a Tracery grammar to write sentences that include the isolated parts. Because the parts have been labelled using spaCy, you can be reasonbly sure that they'll fit into particular slots in the sentence. (I used a similar technique for my [Cheap Space Nine](https://twitter.com/cheapspacenine) bot.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "86LHCp94bzeK"
   },
   "outputs": [],
   "source": [
    "import tracery\n",
    "from tracery.modifiers import base_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "vEN6VX-LbzeK"
   },
   "outputs": [],
   "source": [
    "rules = {\n",
    "    \"subject\": [w.text for w in subjects],\n",
    "    \"object\": [w.text for w in objects],\n",
    "    \"verb\": [w.text for w in past_tense_verbs if w.text not in ('was', 'were', 'went')], # exclude common irregular verbs\n",
    "    \"adj\": [w.text for w in adjs],\n",
    "    \"people\": [w.text for w in people],\n",
    "    \"loc\": [w.text for w in locations],\n",
    "    \"time\": [w.text for w in times],\n",
    "    \"origin\": \"#scene#\\n\\n[charA:#subject#][charB:#subject#][prop:#object#]#sentences#\",\n",
    "    \"scene\": \"SCENE: #loc#, #time.lowercase#\",\n",
    "    \"sentences\": [\n",
    "        \"#sentence#\\n#sentence#\",\n",
    "        \"#sentence#\\n#sentence#\\n#sentence#\",\n",
    "        \"#sentence#\\n#sentence#\\n#sentence#\\n#sentence#\"\n",
    "    ],\n",
    "    \"sentence\": [\n",
    "        \"#charA.capitalize# #verb# #prop#.\",\n",
    "        \"#charB.capitalize# #verb# #prop#.\",\n",
    "        \"#prop.capitalize# became #adj#.\",\n",
    "        \"#charA.capitalize# and #charB# greeted each other.\",\n",
    "        \"'Did you hear about #object.lowercase#?' said #charA#.\",\n",
    "        \"'#subject.capitalize# is #adj#,' said #charB#.\",\n",
    "        \"#charA.capitalize# and #charB# #verb# #object#.\",\n",
    "        \"#charA.capitalize# and #charB# looked at each other.\",\n",
    "        \"#sentence#\\n#sentence#\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "f2Ke1MYkbzeL"
   },
   "outputs": [],
   "source": [
    "grammar = tracery.Grammar(rules)\n",
    "grammar.add_modifiers(base_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "s67UhKUMbzeL",
    "outputId": "8f55ab33-577d-44a5-ef5b-c905063bbd81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENE: Europe, one night\n",
      "\n",
      "'Did you hear about him?' said he.\n",
      "The whole day became private.\n",
      "He and Dean Clinton looked at each other.\n",
      "He and Dean Clinton had his grandmother.\n",
      "\n",
      "SCENE: Kimberly, a night\n",
      "\n",
      "Doris Attinger and them looked at each other.\n",
      "Them became the principal.\n",
      "\n",
      "SCENE: Earth, just a few hours\n",
      "\n",
      "'Did you hear about sally?' said her.\n",
      "'An infatuated Doris is special,' said Tony.\n",
      "Her and Tony looked at each other.\n",
      "Tony took a wager.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(grammar.flatten(\"#origin#\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M8g2y2ubzeL"
   },
   "source": [
    "## Markov chain text generation\n",
    "\n",
    "Another way to produce new narratives from existing narrative text is to find statistical patterns in the text itself and then make the computer create new text that follows those statistical patterns. Markov chain text generation has been a pastime of poets and programmers going back [all the way to 1983](https://www.jstor.org/stable/24969024), so it should be no surprise that there are many implementations of the idea in Python that you can download and install. The one we're going to use is [Markovify](https://github.com/jsvine/markovify), a Markov chain text generation library originally developed for BuzzFeed, apparently. Writing [code to implement a Markov chain generator](https://github.com/aparrish/rwet/blob/master/ngrams-and-markov-chains.ipynb) on your own is certainly possible, but Markovify comes with a lot of extra niceties that will make our lives easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zbobhYnbzeL"
   },
   "source": [
    "To install Markovify on your computer, run the cell below. (You can skip this step if you're using this notebook in Binder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mPWVkkdbzeM",
    "outputId": "a90b15a2-ebad-464d-bac6-d2547de228d6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markovify in /Users/allison/anaconda/lib/python3.6/site-packages (0.7.1)\n",
      "Requirement already satisfied: unidecode in /Users/allison/anaconda/lib/python3.6/site-packages (from markovify) (1.0.22)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxub7HMMbzeM"
   },
   "source": [
    "And then run this cell to make the library available in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "e8fSTipKbzeM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPFNqMPgbzeM"
   },
   "source": [
    "We need a list of strings to train the Markov generator. For now, let's just get all of the sentences from any movie in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "3QtPE1VAbzeM"
   },
   "outputs": [],
   "source": [
    "all_text = [item['text'] for item in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGj22AvVbzeN"
   },
   "source": [
    "The code in the following cell creates a new text generator, using the text in the variable specified to build the Markov model, which is then assigned to the variable `all_text_gen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "I7BFTQVlbzeN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_text_gen = markovify.Text(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7thjhqxebzeN"
   },
   "source": [
    "You can then call the `.make_sentence()` method to generate a sentence from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "PTWjeh7VbzeO",
    "outputId": "6f530aea-4615-4a6a-ff3c-4d1f0855e80e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Izzy promises not to tell him it looks like the Bachelor TV show, restoring his fortunes in the children's home.\n"
     ]
    }
   ],
   "source": [
    "print(all_text_gen.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W92FrZnRbzeO"
   },
   "source": [
    "The `.make_short_sentence()` method allows you to specify a maximum length for the generated sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "C8dkqPaEbzeP",
    "outputId": "c2b70fcc-c3d8-4272-c14b-24eb395fb7ea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He however, lives with his efficiency plan.\n"
     ]
    }
   ],
   "source": [
    "print(all_text_gen.make_short_sentence(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gMvFjImbzeP"
   },
   "source": [
    "By default, Markovify tries to generate a sentence that is significantly different from any existing sentence in the input text. As a consequence, sometimes the `.make_sentence()` or `.make_short_sentence()` methods will return `None`, which means that in ten tries it wasn't able to generate such a sentence. You can work around this by increasing the number of times it tries to generate a sufficiently unique sentence using the `tries` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "1aYjFKdcbzeP",
    "outputId": "151538f8-4a2f-4cfc-c89a-9a8b89a45f98",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The men and marry her.\n"
     ]
    }
   ],
   "source": [
    "print(all_text_gen.make_short_sentence(40, tries=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiJPKn_hbzeQ"
   },
   "source": [
    "Or by disabling the check altogether with `test_output=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "YOW0oY_5bzeQ",
    "outputId": "99c4e9fa-9997-4d29-91fb-60b4e7194d43",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eddie immediately becomes jealous.\n"
     ]
    }
   ],
   "source": [
    "print(all_text_gen.make_short_sentence(40, test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBFBG5R1bzeR"
   },
   "source": [
    "### Changing the order\n",
    "\n",
    "When you create the model, you can specify the order of the model using the `state_size` parameter. It defaults to 2. Let's make two model with different orders and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "v2Y3-SiqbzeR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_1 = markovify.Text(all_text, state_size=1)\n",
    "gen_4 = markovify.Text(all_text, state_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "phV1aO0fbzeR",
    "outputId": "b0853aad-fa4a-48ed-830e-a7e0bdd9359c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1\n",
      "Ely decides to mend, first plane to seduce Maggie, Dick, who almost did to be there.\n",
      "\n",
      "order 4\n",
      "dr Catchadourian persuades Luke to go through a full transformation.\n"
     ]
    }
   ],
   "source": [
    "print(\"order 1\")\n",
    "print(gen_1.make_sentence(test_output=False))\n",
    "print()\n",
    "print(\"order 4\")\n",
    "print(gen_4.make_sentence(test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIjB-yIDbzeS"
   },
   "source": [
    "In general, the higher the order, the more the sentences will seem \"coherent\" (i.e., more closely resembling the source text). Lower order models will produce more variation. Deciding on the order is usually a matter of taste and trial-and-error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KHGerQUbzeS"
   },
   "source": [
    "### Changing the level\n",
    "\n",
    "Markovify, by default, works with *words* as the individual unit. It doesn't come out-of-the-box with support for character-level models. The following code defines a new kind of Markovify generator that implements character-level models. Execute it before continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "SdziITGabzeS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SentencesByChar(markovify.Text):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-6BJ12wbzeS"
   },
   "source": [
    "Any of the parameters you passed to `markovify.Text` you can also pass to `SentencesByChar`. The `state_size` parameter still controls the order of the model, but now the n-grams are characters, not words.\n",
    "\n",
    "The following cell implements a character-level Markov text generator for the word \"condescendences\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Qy9CjgsIbzeS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "con_model = SentencesByChar(\"condescendences\", state_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhV1p4dGbzeS"
   },
   "source": [
    "Execute the cell below to see the output—it'll be a lot like what we implemented by hand earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "yABpG8YIbzeT",
    "outputId": "13395784-f724-4c22-f591-7fccd632a1c9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'condescendescencendes'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_model.make_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrTWftl0bzeT"
   },
   "source": [
    "Of course, you can use a character-level model on any text of your choice. So, for example, the following cell creates a character-level order-7 Markov chain text generator from text A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "rqWqM0a0bzeT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_char = SentencesByChar(all_text, state_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICRQwWS1bzeU"
   },
   "source": [
    "And the cell below prints out a random sentence from this generator. (The `.replace()` is to get rid of any newline characters in the output.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "VNrMKfE4bzeU",
    "outputId": "9a2490dc-0ebd-47a4-f78d-3a17ca161c36",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They escapes they are conversation with Joy on her.\n"
     ]
    }
   ],
   "source": [
    "print(gen_char.make_sentence(test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQHOorXIbzeU"
   },
   "source": [
    "### Thinking about structure\n",
    "\n",
    "It's one thing to be able to produce one plausible sentence of a plot summary using Markov chains, but another to create a sense of overall structure between sentences, and generating narratives with these kinds of long-term dependencies is still an open problem in computational creativity. The approach I'm going to suggest below relies on the intuition that sentences in a plot summary share characteristics based on their position in the summary. First sentences will generally introduce characters and present an initial situation; last sentences will generally describe how the situation was resolved; and sentences in between will describe developing action.\n",
    "\n",
    "Following this intuition, let's create *three different Markov chains*: one for beginning sentences, one for middle sentences, and one for final sentences. We can use the `index` of each sentence in our corpus to give us this information.\n",
    "\n",
    "First, the beginnings are lines whose index is zero (i.e., they're the first sentence for this plot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "CXkkQYWxbzeU"
   },
   "outputs": [],
   "source": [
    "beginnings = [line['text'] for line in sentences if line['index'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "-189QYC9bzeV",
    "outputId": "84226b8e-9a5e-4cd4-b06d-822e27cdab8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eddie \"Gonzo\" Gilman is the head geek at his high school—and determined to do something about it.',\n",
       " 'Leslie Wright (Queen Latifah) is a straight-shooting physical therapist and die-hard basketball fan who is tired of being a guy\\'s best friend or as Morgan called her the \"homegirl\" type.',\n",
       " 'Estlin is a journalist in New York City, fresh out of college and ready to make a difference in an all-too cynical world.',\n",
       " 'Shante Smith (Vivica Fox) is a woman who gives advice on how to keep a man in check.',\n",
       " 'Megan (Keira Knightley) is an aimless twenty-eight-year-old who is in a committed relationship with her high school sweetheart and is still close with her high school friends.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(beginnings, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyrsTvK_bzeV"
   },
   "source": [
    "And endings are sentences that come last in the plot (i.e., their index is one less than the total number of sentences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "sQSAQ7eXbzeV"
   },
   "outputs": [],
   "source": [
    "endings = [line['text'] for line in sentences if line['index'] == line['total'] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "mCATvHDQbzeV",
    "outputId": "4515629d-bf2e-40a1-8d29-3020fb32100c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cody and Natalie are now a couple and the film ends with them sharing their first kiss.',\n",
       " 'So love is reborn.',\n",
       " 'Mimi takes pity on him and a romance blooms.',\n",
       " \"The film ends with Annie and Crash dancing in Annie's candle-lit living room.\",\n",
       " 'As she takes control of her life, Sara faces the challenge of discovering what she really wants, so that she can make the best move of her life.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(endings, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcbBGL2sbzeW"
   },
   "source": [
    "And \"middles\" are anything in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "u8zamV3NbzeW"
   },
   "outputs": [],
   "source": [
    "middles = [line['text'] for line in sentences if 0 < line['index'] < line['total'] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Ca9K3gIwbzeW",
    "outputId": "27a10654-45b8-46f5-a292-4798f0b0a151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At the head of the list are the twin daughters of the Earl of Stokeshire (George Grossmith, Jr), Lady Mary Rose (Wendy Barrie) and Lady Rose Mary (Joan Gardner).',\n",
       " \"Cody's parents pick him up, none the wiser about his dangerous exploits.\",\n",
       " \"Madea takes Helen in and helps her get back on her feet, to the dismay of Madea's brother, Joe.\",\n",
       " 'Richard lets Julia take over the house for a couple of hours, in which she \"straightens out\" the household.',\n",
       " \"Eventually, Jamal is given a chance to return to the men's league, but the hearing takes place at the same time as the Banshee's first playoff game.\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(middles, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97oRqWuebzeX"
   },
   "source": [
    "The following cell creates the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "fCUVSmDibzeY"
   },
   "outputs": [],
   "source": [
    "beginning_gen = markovify.Text(beginnings)\n",
    "middle_gen = markovify.Text(middles)\n",
    "ending_gen = markovify.Text(endings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpCpvX5ibzeZ"
   },
   "source": [
    "Now you can generate tiny narratives by producing a beginning sentence, a middle sentence, and an ending sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "i0efbBnvbzea",
    "outputId": "a78dabe1-3ec2-4c6b-c3e2-8010371ead36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A story of two 1940-style cars in the waters of Santa Catalina Island, California.\n",
      "Eventually, the calendar is successful and stylish.\n",
      "Fallon takes the bet, whereupon the woman who seemingly follows his manly ideas about dating.\n"
     ]
    }
   ],
   "source": [
    "print(beginning_gen.make_short_sentence(100))\n",
    "print(middle_gen.make_short_sentence(100))\n",
    "print(ending_gen.make_short_sentence(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9T9oEV1bzea"
   },
   "source": [
    "The narratives still feel disconnected (and there are often jarring mismatches in pronoun antecedents), but the artifacts produced with this method do feel a bit narrative-like? Maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4rAxzOlbzea"
   },
   "source": [
    "### Combining models\n",
    "\n",
    "Markovify has a handy feature that allows you to *combine* models, creating a new model that draws on probabilities from both of the source models. You can use this to create hybrid output that mixes the style and content of two (or more!) different source texts. To do this, you need to create the models independently, and then call `.combine()` to combine them.\n",
    "\n",
    "The code below combines models for beginning sentences, middle sentences, and ending sentences into one model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "jp0lTn3Cbzea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combo = markovify.combine([beginning_gen, middle_gen, ending_gen], [10, 1, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wqz1Aisbzea"
   },
   "source": [
    "The bit of code `[10, 1, 10]` controls the \"weights\" of the models, i.e., how much to emphasize the probabilities of any model. You can change this to suit your tastes. (E.g., if you want mostly beginnings with but a bit of middles and a *soupçon* of ends, try `[10, 2, 1]`.)\n",
    "\n",
    "Then you can create sentences using the combined model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "v0UEP45Nbzea",
    "outputId": "010023a4-0104-4e0d-e938-0282d16736b9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager to teach Amarilly high class bakery, and get into an argument ensues.\n"
     ]
    }
   ],
   "source": [
    "print(combo.make_short_sentence(120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNn-wpVWbzeb"
   },
   "source": [
    "## Prepping the corpus for fine-tuning a large language model\n",
    "\n",
    "Markov chains are cheap and fun, but they don't do a great job of the one thing we expect from stories: maintaining coherence over a long stretch of text. Accomplishing this is a more difficult task, and requires making use of more sophisticated machine learning models, belonging to the category of large pre-trained neural networks. These models are fundamentally similar to Markov chains, in that they make a prediction about what will come next in a text, given some stretch of context. Unlike a Markov chain, a large pre-trained neural network can predict what will come next in a text, even if the context you give it has never been seen in the training text. It can also work on contexts of arbitrary and variable length. Handy!\n",
    "\n",
    "These language models are already trained on a large amount of text. Generally, you don't train them from scratch on your own, but instead \"fine-tune\" them to bring their probabilities more in line with a particular source text.\n",
    "\n",
    "One such model, [OpenAI's GPT-2](https://github.com/openai/gpt-2) does a pretty good job of maintaining long-distance coherence, and it's easy to fine-tune the model with Max Woolf's [aitextgen](https://github.com/minimaxir/aitextgen/).  We'll use the [example Colab notebook](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD?usp=sharing) from the aitextgen repository. This notebook works best when it's fine-tuned on text in a prose format. The model can also learn ad-hoc markup elements that you add to the text. We'll use this feature of the model to make it possible to generate stories from beginning to end, by adding a `[BEGIN STORY]` marker before each story in the source text, followed by the title of the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "bNVk01jrbzeb"
   },
   "outputs": [],
   "source": [
    "out = []\n",
    "last_title = None\n",
    "for sent in sentences[:10000]:\n",
    "    if sent['title'] != last_title:\n",
    "        out.append(\"\")\n",
    "        out.append(\"[BEGIN STORY]\")\n",
    "        out.append(sent['title'])\n",
    "        out.append(\"\")\n",
    "        last_title = sent['title']\n",
    "    out.append(sent['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUV_pVF_bzeb"
   },
   "source": [
    "Here's what the data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "zmdLJn_1bzeb",
    "outputId": "81d750da-233f-4350-c17a-b516dcf98ae4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[BEGIN STORY]',\n",
       " 'Four Weddings and a Funeral',\n",
       " '',\n",
       " 'The film follows the adventures of a group of friends through the eyes of Charles, a good-natured but socially awkward man living in London, who becomes smitten with Carrie, an American whom Charles keeps meeting at four weddings and a funeral.',\n",
       " 'The first wedding is that of Angus and Laura, at which Charles is the best man.',\n",
       " 'Charles and his single friends wonder whether they will ever get married.',\n",
       " 'Charles meets Carrie and spends the night with her.',\n",
       " 'Carrie pretends that, now they have slept together, they will have to get married, to which Charles endeavours to respond before realising she is joking.',\n",
       " 'Carrie observes that they may have missed an opportunity and then returns to America.',\n",
       " 'The second wedding is that of Bernard and Lydia, a couple who became romantically involved at the previous wedding.',\n",
       " 'Charles encounters Carrie again, but she introduces him to her fiancé, Sir Hamish Banks, a wealthy politician.',\n",
       " 'At the reception, Charles finds himself seated with several ex-girlfriends who relate embarrassing stories about his inability to be discreet and afterwards bumps into Henrietta, known among Charles\\' friends as \"Duckface\", with whom he had a particularly difficult relationship.',\n",
       " 'Charles retreats to an empty hotel suite, seeing Carrie and Hamish leave in a taxicab, only to be trapped in a cupboard after the newlyweds stumble into the room to have sex.',\n",
       " 'After Charles awkwardly exits the room, Henrietta confronts him about his habit of \"serial monogamy\", telling him he is afraid of letting anyone get too close to him.',\n",
       " 'Charles then runs into Carrie, and they end up spending another night together.',\n",
       " \"A month later, Charles receives an invitation to Carrie's wedding.\",\n",
       " 'While shopping for a present, he coincidentally encounters Carrie and ends up helping her select her wedding dress.',\n",
       " 'Carrie lists her more than thirty sexual partners.',\n",
       " 'Charles later awkwardly tries confessing his love to her and hinting that he would like to have a relationship with her, to no avail.',\n",
       " 'The third wedding is that of Carrie and Hamish.',\n",
       " 'Charles attends, depressed at the prospect of Carrie marrying Hamish.',\n",
       " \"At the reception, Gareth instructs his friends to seek potential mates; Fiona's brother, Tom, stumbles through an attempt to connect with a woman until she reveals that she is the minister's wife, while Charles's flatmate, Scarlett, strikes up a conversation with an American named Chester.\",\n",
       " 'As Charles watches Carrie and Hamish dance, Fiona deduces his feelings about Carrie.',\n",
       " 'When Charles asks why Fiona is not married, she confesses that she has loved Charles since they first met years earlier.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5-izP45bzec"
   },
   "source": [
    "The following cell writes this out to a file, which you can then upload to the aitextgen notebook on Google Colab to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Hwc-gtsAbzec"
   },
   "outputs": [],
   "source": [
    "with open(\"story_training.txt\", \"w\") as fh:\n",
    "    fh.write(\"\\n\".join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5D41I4Bbzec"
   },
   "source": [
    "In the text generation section of that notebook, try prompting the model with `[BEGIN STORY]` followed by the title of a story you'd like to generate!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "corpus-driven-narrative-generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
