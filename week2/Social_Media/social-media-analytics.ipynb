{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "c44d8d5589f53abf8acf4a9abc42a9c8a567fd25"
   },
   "outputs": [],
   "source": [
    "# If tweepy is not installed in your current environment\n",
    "! pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import tweepy #The Twitter API\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "#from textblob import TextBlob #For Sentiment Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "#Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# tweets = pd.read_csv('tweets_all.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "0326aacd24769de9c4db4e695df4819ae992e896"
   },
   "outputs": [],
   "source": [
    "consumer_key = '6i7pvwjFLtXNcecug1xetUSqf'\n",
    "consumer_secret = '5jGK2zlTgKnk90i5MXRcmpOpt1qvBWSsfsst6EqkO7VS1mPa1W'\n",
    "access_token = '137590128-Qw9foqrswzhkV0yQGAVIFVOt7Y5EEW7k73SuyQWc'\n",
    "access_token_secret = 'q47C3TZqmIbBg7TNd2oPDBcCE2d0SDPUdQaJDSsbGMbtq'\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7fa97389269cf16fbcd75e5aaea2267c0883f1b3"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'API' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9cefb5f137df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#datascience'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#**supply whatever query you want here**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'API' object has no attribute 'search'"
     ]
    }
   ],
   "source": [
    "search = api.search(q='#datascience', count=100) #**supply whatever query you want here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f7ab502447f31715b33cb2ba18d9b961bc6764d"
   },
   "outputs": [],
   "source": [
    "searched_tweets = [each._json for each in search]\n",
    "json_strings = [json.dumps(json_obj) for json_obj in searched_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6decc7fc3432686dfa12793c6941451046869f0"
   },
   "outputs": [],
   "source": [
    "searched_tweets[0]['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f245350556c9c558fd7652df1fd9954da774cf59"
   },
   "outputs": [],
   "source": [
    "def flatten_tweets(tweets_json):\n",
    "    \"\"\" Flattens out tweet dictionaries so relevant JSON\n",
    "        is in a top-level dictionary.\"\"\"\n",
    "    tweets_list = []\n",
    "    \n",
    "    # Iterate through each tweet\n",
    "    for tweet in tweets_json:\n",
    "        tweet_obj = json.loads(tweet)\n",
    "    \n",
    "        # Store the user screen name in 'user-screen_name'\n",
    "        tweet_obj['user-screen_name'] = tweet_obj['user']['screen_name']\n",
    "    \n",
    "        # Check if this is a 140+ character tweet\n",
    "        if 'extended_tweet' in tweet_obj:\n",
    "            # Store the extended tweet text in 'extended_tweet-full_text'\n",
    "            tweet_obj['extended_tweet-full_text'] = tweet_obj['extended_tweet']['full_text']\n",
    "    \n",
    "        if 'retweeted_status' in tweet_obj:\n",
    "            # Store the retweet user screen name in 'retweeted_status-user-screen_name'\n",
    "            tweet_obj['retweeted_status-user-screen_name'] = tweet_obj['retweeted_status']['user']['screen_name']\n",
    "\n",
    "            # Store the retweet text in 'retweeted_status-text'\n",
    "            tweet_obj['retweeted_status-text'] = tweet_obj['retweeted_status']['text']\n",
    "            \n",
    "        tweets_list.append(tweet_obj)\n",
    "    return tweets_list\n",
    "\n",
    "tweet= flatten_tweets(json_strings)\n",
    "tweet_d = pd.DataFrame(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27ad2bf81e856a98ae42c717bc58a58de3437352"
   },
   "outputs": [],
   "source": [
    "tweet_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "777f7aa30805369be8a8166ea74b3ad1281882eb"
   },
   "outputs": [],
   "source": [
    "tweets_with_quoted_status = tweet_d[~tweet_d.quoted_status.isnull()]['quoted_status'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6815126ca532112fddb1cc6cdc00236d463f2ae5"
   },
   "outputs": [],
   "source": [
    "tweet_d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9c2d0ad1a67eb37d9b9abf46186c21e5ac79019"
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame()\n",
    "msgs = []\n",
    "msg =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "450a066f62cdbb67702055b7da36d0c69e54590d"
   },
   "outputs": [],
   "source": [
    "search[0].user.followers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ce0430337e93144939acfede600765486282a25"
   },
   "outputs": [],
   "source": [
    "for tweet in search:\n",
    "    msg = [tweet.text, tweet.source, tweet.source_url, tweet.user.location, tweet.user.followers_count] \n",
    "    msg = tuple(msg)                    \n",
    "    msgs.append(msg)\n",
    "    \n",
    "tweets = pd.DataFrame(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc9a94e6beb2d55aeefbd5baf0ccee80211e6a51"
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "289aad4e2c7a0421ad04e602a01121928b564a32"
   },
   "outputs": [],
   "source": [
    "tweets.columns = ['text', 'source', 'url', 'location', 'followers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2fb6625d926c536a1d7d52109c23ff9f7e1c416"
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ded04217018d27c4dad2ecf8c956f8ee4ac7105"
   },
   "outputs": [],
   "source": [
    "tweets['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b07bd984bf61c9bed191f107c0db66917bbc8459"
   },
   "outputs": [],
   "source": [
    "#Preprocessing delete \"RT\" and \"@username\":\n",
    "tweets['tweetos'] = '' \n",
    "\n",
    "#add tweetos first part\n",
    "for i in range(len(tweets['text'])):\n",
    "    try:\n",
    "        tweets['tweetos'][i] = tweets['text'].str.split(' ')[i][0]\n",
    "    except AttributeError:    \n",
    "        tweets['tweetos'][i] = 'other'\n",
    "\n",
    "#Preprocessing tweetos. select tweetos contains 'RT @'\n",
    "for i in range(len(tweets['text'])):\n",
    "    if tweets['tweetos'].str.contains('@')[i]  == False:\n",
    "        tweets['tweetos'][i] = 'other'\n",
    "        \n",
    "# remove URLs, RTs, and twitter handles\n",
    "for i in range(len(tweets['text'])):\n",
    "    tweets['text'][i] = \" \".join([word for word in tweets['text'][i].split()\n",
    "                                if 'http' not in word and '@' not in word and '<' not in word])\n",
    "\n",
    "\n",
    "tweets['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55238cb2ac56cabe0aabb303963191d5db559f41"
   },
   "outputs": [],
   "source": [
    "tweets['text'] = tweets['text'].apply(lambda x: re.sub('[!@#$:).;,?&]', '', x.lower()))\n",
    "tweets['text'] = tweets['text'].apply(lambda x: re.sub('  ', ' ', x))\n",
    "tweets['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fdb00314731095bf72eaef73ffc1ff8a03143b2"
   },
   "outputs": [],
   "source": [
    "def wordcloud(tweets,col):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(background_color=\"white\",stopwords=stopwords,random_state = 2016).generate(\" \".join([i for i in tweets[col]]))\n",
    "    plt.figure( figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Good Morning Datascience+\")\n",
    "wordcloud(tweets,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b81574ed2acab9e05ffd927df86d3a3150e35a8"
   },
   "outputs": [],
   "source": [
    "tweets['location'] = tweets['location'].apply(lambda x: ' ' if x==None else str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62784dd39684cc87d7e0d2e82bf6899ae3e81373"
   },
   "outputs": [],
   "source": [
    "wordcloud(tweets, 'location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06e14508b5d70d08551e9b3e0078e32fc94205ba"
   },
   "outputs": [],
   "source": [
    "tweets['source'][69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e2dd2b51379d4a6a5f0a7fc78bd44ff359cff3a"
   },
   "outputs": [],
   "source": [
    "tweets['source_new'] = ''\n",
    "\n",
    "for i in range(len(tweets['source'])):\n",
    "    m = re.search('(?i)<a([^>]+)>(.+?)</a>', tweets['source'][i])\n",
    "    try:\n",
    "        tweets['source_new'][i]=m.group(0)\n",
    "    except AttributeError:\n",
    "        tweets['source_new'][i]=tweets['source'][i]\n",
    "        \n",
    "tweets['source_new'] = tweets['source_new'].str.replace('', ' ', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24cf71e415567544d879a5c381aa2ffb69367f4e"
   },
   "outputs": [],
   "source": [
    "tweets_by_type = tweets.groupby(['source_new'])['followers_count'].sum()\n",
    "plt.title('Number of followers by Source', bbox={'facecolor':'0.8', 'pad':0})\n",
    "tweets_by_type.transpose().plot(kind='bar',figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dbc3224e368e1de073596558370607bb9691bba9"
   },
   "outputs": [],
   "source": [
    "tweets['source_new2'] = ''\n",
    "\n",
    "for i in range(len(tweets['source_new'])):\n",
    "    if tweets['source_new'][i] not in ['Twitter for Android ','Instagram ','Twitter Web Client ','Twitter for iPhone ']:\n",
    "        tweets['source_new2'][i] = 'Others'\n",
    "    else:\n",
    "        tweets['source_new2'][i] = tweets['source_new'][i] \n",
    "\n",
    "tweets_by_type2 = tweets.groupby(['source_new2'])['followers_count'].sum()\n",
    "tweets_by_type2.rename(\"\",inplace=True)\n",
    "explode = (1,0,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "863ff8e6220bb5750587fa4b4c1a606285ab0bc4"
   },
   "outputs": [],
   "source": [
    "tweets.groupby(['source_new2'])['followers_count'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7083d590e4533520c1deee6502fa86d441350eb9"
   },
   "outputs": [],
   "source": [
    "tweets['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in tweets['text']]       \n",
    "vectorizer = TfidfVectorizer(max_df=0.5,max_features=10000,min_df=10,stop_words='english',use_idf=True)\n",
    "X = vectorizer.fit_transform(tweets['text_lem'].str.upper())\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "tweets['sentiment_compound_polarity']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['compound'])\n",
    "tweets['sentiment_neutral']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neu'])\n",
    "tweets['sentiment_negative']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neg'])\n",
    "tweets['sentiment_pos']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['pos'])\n",
    "tweets['sentiment_type']=''\n",
    "tweets.loc[tweets.sentiment_compound_polarity>0,'sentiment_type']='POSITIVE'\n",
    "tweets.loc[tweets.sentiment_compound_polarity==0,'sentiment_type']='NEUTRAL'\n",
    "tweets.loc[tweets.sentiment_compound_polarity<0,'sentiment_type']='NEGATIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57f2169c9b24483fdcaffecc7f1ad857b9425897"
   },
   "outputs": [],
   "source": [
    "tweets_sentiment = tweets.groupby(['sentiment_type'])['sentiment_neutral'].count()\n",
    "tweets_sentiment.rename(\"\",inplace=True)\n",
    "explode = (1, 0, 0)\n",
    "plt.subplot(221)\n",
    "tweets_sentiment.transpose().plot(kind='barh',figsize=(20, 20))\n",
    "plt.title('Sentiment Analysis 1')\n",
    "plt.subplot(222)\n",
    "tweets_sentiment.plot(kind='pie',figsize=(20, 20),autopct='%1.1f%%',shadow=True,explode=explode)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=3, borderaxespad=0.)\n",
    "plt.title('Sentiment Analysis 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c61254775590f61c86ce70644cf7767e5dcb1e80"
   },
   "outputs": [],
   "source": [
    "tweets[tweets.sentiment_type == 'NEGATIVE'].text.reset_index(drop = True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8af1e29dd8ef11cf1ffd1d3a40c2c6fa13d1e613"
   },
   "outputs": [],
   "source": [
    "sid.polarity_scores(\"rt will you be at mozfest this weekend don’t miss our interactive conversation about youthsurveillance with\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "430f10d5326e5feb58eec3b28e2a977befd35764"
   },
   "outputs": [],
   "source": [
    "# Warning: only 1-3% of Twitter data have geographical data\n",
    "import pycountry\n",
    "country = {}\n",
    "for i in list(pycountry.countries):\n",
    "    country[i.alpha_2] = i.name\n",
    "    \n",
    "loc = []\n",
    "for i in tweets[\"location\"]:\n",
    "    j = i.split(\",\")\n",
    "    if len(j)==1:\n",
    "        if j[0].strip() in country.keys():\n",
    "            loc.append(country[j[0].strip()])\n",
    "        elif j[0].strip() in country.values():\n",
    "            loc.append(j[0])\n",
    "    else:\n",
    "        if j[len(j)-1].strip() in country.keys():\n",
    "            loc.append(country[j[len(j)-1].strip()])\n",
    "        elif j[len(j)-1].strip() in country.values():\n",
    "            loc.append(j[len(j)-1])\n",
    "            \n",
    "for i in range(len(loc)):\n",
    "    loc[i] = loc[i].strip()\n",
    "    \n",
    "loc = list(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1984b5641646114a0f65e7d482746ea688a99214"
   },
   "outputs": [],
   "source": [
    "unique_loc = list(set(loc))\n",
    "\n",
    "c = []\n",
    "for i in unique_loc:\n",
    "    c.append(loc.count(i))\n",
    "    \n",
    "q = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3abeb900f2e35d3097c42aad84713f4635e40bd"
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
    "latitude = [] \n",
    "long = []\n",
    "for i in unique_loc:\n",
    "    if i != None:\n",
    "        location = geolocator.geocode(i)\n",
    "        if location!=None:\n",
    "            latitude.append(location.latitude)#, location.longitude)\n",
    "            long.append(location.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "987bc6130ec8409b95676ab7459a5ec5a2fe54f2"
   },
   "outputs": [],
   "source": [
    "tweets['latitude'] = pd.Series(latitude)\n",
    "tweets['longitude'] = pd.Series(long)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27bb1c08ba4a64b821358909eb7f5fce12545529"
   },
   "outputs": [],
   "source": [
    "q = pd.DataFrame({\"latitude\":latitude,\"longitude\":long,\"location\":unique_loc,\"count\":c})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97270ff6e21f5ad3c9ee1b1fde849ed9bdd36439"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "m = folium.Map(location=[20, 0], tiles=\"Mapbox Bright\", zoom_start=2)\n",
    "for i in range(0,len(q)):\n",
    "    popup= folium.Popup(q.iloc[i]['location'], parse_html=True)\n",
    "    folium.Marker([q.iloc[i]['latitude'], q.iloc[i]['longitude']], popup=popup).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "395587affa9f0b20f9d44f5b845788533a01b014"
   },
   "outputs": [],
   "source": [
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
